bundle_root: null                           # root folder of the fold
ckpt_path: $@bundle_root + '/model'         # location to save checkpoints and logs
mlflow_tracking_uri: $@ckpt_path + '/mlruns/'
mlflow_experiment_name: "Auto3DSeg"

data_file_base_dir: null                    # location of the dataset
data_list_file_path: null                   # location of the file with a list of files image/label in the dataset

modality: mri                               # main image modality, must be one of mri, ct, pet
fold: 0                                     # fold number

class_names: null                           # names of the classes for a classification task -> null for segmentation
class_index: null                           # indices of the classes for a classification task -> null for segmentation

debug: false
ckpt_save: true
cache_rate: 0                            # rate at which data is cached during training/val
roi_size: [128, 128, 128]                   # might be changed to patch_size
roi_size_valid: null
random_seed: 0

adapt_valid_mode: true
adapt_valid_progress_percentages: [10, 40, 70]
adapt_valid_num_epochs_per_validation: [5, 5, 5]

auto_scale_allowed: true                    # indicating whether auto-scaling (hence, normalization) is allowed
auto_scale_batch: true
auto_scale_roi: false
auto_scale_filters: false
auto_scale_max_epochs: 1000                 # maximum number of epochs for auto-scaling.


quick: false                                # whether to run a quick version of the model for testing or debugging
channels_last: true
validate_final_original_res: true           # whether to validate the model at the original resolution of the input data
calc_val_loss: false                        # indicating whether to calculate the validation loss
log_output_file: "$@bundle_root + '/model/training.log'"
cache_class_indices: null                   # indices of the classes to be cached during training

stop_on_lowacc: false                       # stop training if the accuracy is too low
                               
train_cache_rate: "$@cache_rate"            # rate at which training data is cached. Same as cache_rate by default
validate_cache_rate: "$@cache_rate"         # rate at which validation data is cached
show_cache_progress: false

use_pretrain: false

training:
  # hyper-parameters
  amp: true                                     # automatic mixed precision is used for training -Â° speed up training, reduce GPU memory usage
  determ: false
  early_stopping_fraction: 0.001
  early_stop_mode: true
  early_stop_delta: 0
  early_stop_patience: 5
  input_channels: null
  learning_rate: 0.0002
  num_images_per_batch: 1
  num_iterations: 20000
  num_iterations_per_validation: 500
  num_patches_per_image: 1
  num_patches_per_iter: 1     # maybe see how this should be set
  num_sw_batch_size: "$@training#num_patches_per_iter"
  output_classes: null
  n_cases: null
  overlap_ratio: 0.125
  overlap_ratio_final: 0.625
  patch_size: null
  patch_size_valid: null
  pin_memory: false    # was true 
  softmax: true
  sw_input_on_cpu: false
  valid_at_orig_resolution_at_last: true
  valid_at_orig_resolution_only: false

  resample_resolution: null

  transforms:
    resample_resolution: "$@training#resample_resolution"
    lazy_resampling: false

  loss:
    _target_: DiceCELoss
    include_background: true
    squared_pred: true
    smooth_nr: 0
    smooth_dr: 1.0e-05
    softmax: $@training#softmax
    sigmoid: $not @training#softmax
    to_onehot_y: $@training#softmax
    #batch: true

  optimizer:
    _target_: torch.optim.AdamW
    lr: '@training#learning_rate'
    weight_decay: 1.0e-05

  lr_scheduler:
    _target_: monai.optimizers.WarmupCosineSchedule
    optimizer: "$@training#optimizer"
    warmup_steps: $@num_epochs // 100
    t_total: '$@num_epochs // @num_epochs_per_validation + 1'
    warmup_multiplier: 0.1

batch_size: '@training#num_images_per_batch'
num_epochs: 300
num_warmup_epochs: 3
resample: false
resample_resolution: [1, 1, 1]
crop_mode: ratio
normalize_mode: meanstd
intensity_bounds: null

num_epochs_per_validation: 5
num_epochs_per_saving: 1
num_workers: 8
num_workers_validation: 2
num_cache_workers: 4
num_steps_per_image: null
num_crops_per_image: 1

# fine-tuning
finetune:
  activate: false
  pretrained_ckpt_name: "$@bundle_root + '/model_fold' + str(@fold) + '/best_metric_model.pt'"
  #enabled: false
  #ckpt_name: $@bundle_root + '/model/model.pt'


# validation
validate:
  ckpt_name: "$@bundle_root + '/model_fold' + str(@fold) + '/best_metric_model.pt'"
  save_mask: true # before: false 
  log_output_file: "$@bundle_root + '/model_fold' + str(@fold) + '/validation.log'"
  output_path: "$@bundle_root + '/prediction_fold' + str(@fold)" #  $@bundle_root + '/prediction_validation' before
  # invert: true
  # enabled: false


# inference
infer:
  ckpt_name: "$@bundle_root + '/model_fold' + str(@fold) + '/best_metric_model.pt'" # $@bundle_root + '/model/model.pt' before
  fast: false
  data_list_key: testing
  log_output_file: "$@bundle_root + '/model_fold' + str(@fold) + '/inference.log'"
  output_path: "$@bundle_root + '/prediction_' + @infer#data_list_key"
  #enabled: false



