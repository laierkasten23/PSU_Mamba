{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric, ConfusionMatrixMetric\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Dice: 0.8194468529136093\n",
      "Mean Hausdorff Distance: 2.2686533442250005\n",
      "Precision: tensor([0.8804])\n",
      "Recall: tensor([0.7432])\n",
      "F1 Score: tensor([0.8051])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_folder = \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/thesis_experiments/01_aschoplex_from_scratch/working_directory_T1_240820_1823/ensemble_output/image_Ts\"\n",
    "gt_folder = \"/home/linuxlia/Lia_Masterthesis/data/reference_labels/ref_labelTs\"\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(pred_folder) if f.endswith('.nii.gz')])\n",
    "gt_files = sorted([f for f in os.listdir(gt_folder) if f.endswith('.nii')])\n",
    "\n",
    "predictions = [nib.load(os.path.join(pred_folder, f)).get_fdata() for f in pred_files]\n",
    "ground_truths = [nib.load(os.path.join(gt_folder, f)).get_fdata() for f in gt_files]\n",
    "\n",
    "# Initialize the metric\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False, percentile=95)\n",
    "confusion_matrix_metric = ConfusionMatrixMetric(include_background=False, metric_name=[\"precision\", \"recall\", \"f1_score\"], reduction=\"mean\")\n",
    "\n",
    "\n",
    "dice_scores = []\n",
    "hd_distances = []\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Compute metrics for each pair of prediction and ground truth\n",
    "for pred, gt in zip(predictions, ground_truths):\n",
    "    pred_tensor = torch.tensor(pred).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimension\n",
    "    gt_tensor = torch.tensor(gt).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimension\n",
    "    \n",
    "    dice_score = dice_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "    #print(\"dice_metric\", dice_score)\n",
    "    mean_dice_score = dice_score.mean().item()\n",
    "    dice_scores.append(mean_dice_score)\n",
    "\n",
    "    # Compute Hausdorff distance\n",
    "    hd_distance = hausdorff_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "    #print(\"hausdorff_metric\", hd_distance)\n",
    "    hd_distances.append(hd_distance.item())\n",
    "    \n",
    "\n",
    "    # Compute Precision, Recall, and F1 Score\n",
    "    confusion_matrix_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "    precision, recall, f1_score = confusion_matrix_metric.aggregate()\n",
    "\n",
    "    #print(precision, recall, f1_score)\n",
    "    f1_scores.append(f1_score)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "\n",
    "mean_dice = sum(dice_scores) / len(dice_scores)\n",
    "mean_hd_distance = sum(hd_distances) / len(hd_distances)\n",
    "precision = sum(precisions) / len(precisions)\n",
    "recall = sum(recalls) / len(recalls)\n",
    "f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Mean Dice: {mean_dice}\")\n",
    "print(f\"Mean Hausdorff Distance: {mean_hd_distance}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Mean Dice  Mean Hausdorff Distance  Precision    Recall  F1 Score\n",
      "0  image_Ts   0.819447                 2.268653   0.880423  0.743209  0.805111\n",
      "1  image_Ts   0.874529                 1.494537   0.895855  0.784857  0.836601\n",
      "2  image_Ts   0.822920                 2.095111   0.874690  0.816498  0.844546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric, ConfusionMatrixMetric\n",
    "import pandas as pd\n",
    "\n",
    "# List of prediction folders\n",
    "pred_folders = [\n",
    "    \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/thesis_experiments/01_aschoplex_from_scratch/working_directory_T1_240820_1823/ensemble_output/image_Ts\",\n",
    "    \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/thesis_experiments/01_aschoplex_from_scratch/working_directory_T1xFLAIR_240822/ensemble_output/image_Ts\",\n",
    "    \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/thesis_experiments/01_aschoplex_from_scratch/working_directory_FLAIR_240823/ensemble_output/image_Ts\",\n",
    "]\n",
    "\n",
    "gt_folder = \"/home/linuxlia/Lia_Masterthesis/data/reference_labels/ref_labelTs\"\n",
    "gt_files = sorted([f for f in os.listdir(gt_folder) if f.endswith('.nii')])\n",
    "ground_truths = [nib.load(os.path.join(gt_folder, f)).get_fdata() for f in gt_files]\n",
    "\n",
    "# Initialize the metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False, percentile=95)\n",
    "confusion_matrix_metric = ConfusionMatrixMetric(include_background=False, metric_name=[\"precision\", \"recall\", \"f1_score\"], reduction=\"mean\")\n",
    "\n",
    "# List to store the results\n",
    "results_list = []\n",
    "\n",
    "# Loop over each prediction folder\n",
    "for pred_folder in pred_folders:\n",
    "    pred_files = sorted([f for f in os.listdir(pred_folder) if f.endswith('.nii.gz')])\n",
    "    predictions = [nib.load(os.path.join(pred_folder, f)).get_fdata() for f in pred_files]\n",
    "\n",
    "    dice_scores = []\n",
    "    hd_distances = []\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    # Compute metrics for each pair of prediction and ground truth\n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "        gt_tensor = torch.tensor(gt, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Check for empty tensors\n",
    "        if torch.sum(pred_tensor) == 0 or torch.sum(gt_tensor) == 0:\n",
    "            print(\"Warning: Empty prediction or ground truth tensor detected.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure matching shapes\n",
    "        if pred_tensor.shape != gt_tensor.shape:\n",
    "            print(f\"Error: Shape mismatch - pred: {pred_tensor.shape}, gt: {gt_tensor.shape}\")\n",
    "            continue\n",
    "        \n",
    "        # Compute Dice score\n",
    "        dice_score = dice_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "        mean_dice_score = dice_score.mean().item()\n",
    "        dice_scores.append(mean_dice_score)\n",
    "        \n",
    "        # Compute Hausdorff distance\n",
    "        hd_distance = hausdorff_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "        hd_distances.append(hd_distance.item())\n",
    "        \n",
    "        # Accumulate confusion matrix results\n",
    "        confusion_matrix_metric(y_pred=pred_tensor, y=gt_tensor)\n",
    "        precision, recall, f1_score = confusion_matrix_metric.aggregate()\n",
    "        \n",
    "        f1_scores.append(f1_score.item())\n",
    "        precisions.append(precision.item())\n",
    "        recalls.append(recall.item())\n",
    "\n",
    "    # Aggregate metrics to get summary statistics\n",
    "    mean_dice = sum(dice_scores) / len(dice_scores)\n",
    "    mean_hd_distance = sum(hd_distances) / len(hd_distances)\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    mean_recall = sum(recalls) / len(recalls)\n",
    "    mean_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results_list.append({\n",
    "        \"Model\": os.path.basename(pred_folder),\n",
    "        \"Mean Dice\": mean_dice,\n",
    "        \"Mean Hausdorff Distance\": mean_hd_distance,\n",
    "        \"Precision\": mean_precision,\n",
    "        \"Recall\": mean_recall,\n",
    "        \"F1 Score\": mean_f1_score\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the results table\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(predictions[0][..., int(predictions[0].shape[-1] / 2)], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.imshow(ground_truths[0][..., int(ground_truths[0].shape[-1] / 2)], cmap=\"gray\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
