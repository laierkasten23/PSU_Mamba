import os
import shutil
import sys
from copy import deepcopy
from typing import Any, Dict, List
from monai.apps.utils import get_logger
from monai.apps.auto3dseg import DataAnalyzer
from monai.apps.auto3dseg.bundle_gen import BundleAlgo
from monai.apps.auto3dseg.utils import export_bundle_algo_history
from monai.auto3dseg.algo_gen import AlgoGen
from monai.auto3dseg.utils import algo_to_pickle
from monai.bundle.config_parser import ConfigParser
from monai.config import print_config
from monai.utils import ensure_tuple
from monai.utils.enums import AlgoKeys # added to work

print_config()

join=os.path.join

logger = get_logger(module_name=__name__)
ALGO_HASH = os.environ.get("MONAI_ALGO_HASH", "004a63c")

__all__ = ["BundleAlgo", "MyBundleGen"]


# default algorithms
#If a dictionary, must be in the form
#{"algname": dict(_target_="algname.scripts.algo.AlgnameAlgo", template_path="algname"), ...}
#If a list or a string, defines a subset of names of the algorithms to use, e.g. 'segresnet' or
#['segresnet', 'dints'] out of the full set of algorithm templates provided by templates_path_or_url.

default_algos = {
    "UNet3D": dict(_target_="UNet3D.scripts.algo.UNet3D", template_path="UNet3D"),
    "Dints": dict(_target_="Dints.scripts.algo.Dints", template_path="Dints"),
    "DynUnet": dict(_target_="DynUnet.scripts.algo.DynUnet", template_path="DynUnet"),
    
}


class MyBundleGen(AlgoGen):
    """
    This class generates a set of bundles according to the cross-validation folds, each of them can run independently.

    Args:
        algo_path: the directory path to save the algorithm templates. Default is the current working dir.
        algos: dictionary, it outlines the algorithm to use defined in default_algos
        data_stats_filename: the path to the data stats file (generated by DataAnalyzer)
        data_src_cfg_name: the path to the data source config YAML file. The config will be in a form of
            {"modality": "ct", "datalist": "path_to_json_datalist", "dataroot": "path_dir_data"}
    """

    def __init__(self, algo_path: str = ".", algo_template: str = ".", algos=None, data_stats_filename=None, data_src_cfg_name=None):
        self.algos: Any = []

        if algos is None or isinstance(algos, str):
            
            scr_dir=algo_template
            # files=os.listdir(scr_dir)
            new_path=join(algo_path, "algorithm_templates")
            sys.path.insert(0, new_path)
            shutil.copytree(scr_dir, new_path)
            algos = deepcopy(default_algos)
            for name in algos:
                algos[name]["template_path"] = join(
                    algo_path, "algorithm_templates", # algos[name]["template_path"]
                )

        if isinstance(algos, dict):
            for algo_name, algo_params in algos.items():
                try:
                    self.algos.append(ConfigParser(algo_params).get_parsed_content())
                except RuntimeError as e:
                    if "ModuleNotFoundError" in str(e):
                        msg = """Please make sure the folder structure of an Algo Template follows
                            [algo_name]
                            ├── configs
                            │   ├── hyperparameters.yaml  # automatically generated yaml from a set of ``template_configs``
                            │   ├── network.yaml  # automatically generated network yaml from a set of ``template_configs``
                            │   ├── transforms_train.yaml  # automatically generated yaml to define transforms for training
                            │   ├── transforms_validate.yaml  # automatically generated yaml to define transforms for validation
                            │   └── transforms_infer.yaml  # automatically generated yaml to define transforms for inference
                            └── scripts
                                ├── test.py
                                ├── train.py
                                ├── __init__.py
                                └── validate.py
                        """
                        raise RuntimeError(msg) from e
                self.algos[-1].name = algo_name
        else:
            self.algos = ensure_tuple(algos)

        self.data_stats_filename = data_stats_filename
        self.data_src_cfg_filename = data_src_cfg_name
        self.history: List[Dict] = []

    def set_data_stats(self, data_stats_filename: str):  # type: ignore
        """
        Set the data stats filename

        Args:
            data_stats_filename: filename of datastats
        """
        self.data_stats_filename = data_stats_filename

    def get_data_stats(self):
        """Get the filename of the data stats"""
        return self.data_stats_filename

    def set_data_src(self, data_src_cfg_filename):
        """
        Set the data source filename

        Args:
            data_src_cfg_filename: filename of data_source file
        """
        self.data_src_cfg_filename = data_src_cfg_filename

    def get_data_src(self):
        """Get the data source filename"""
        return self.data_src_cfg_filename

    def get_history(self) -> List:  # type: ignore
        """get the history of the bundleAlgo object with their names/identifiers"""
        return self.history

    def generate(self, output_folder=".", num_fold: int = 1): 
        """
        Generate the bundle scripts/configs for each bundleAlgo

        Args:
            output_folder: the output folder to save each algorithm.
            num_fold: the number of cross validation fold
        """
        fold_idx = list(range(num_fold))
        for algo in self.algos:
            for f_id in ensure_tuple(fold_idx):
                # Retrieve the data stats and data source config
                data_stats = self.get_data_stats()
                data_src_cfg = self.get_data_src()
                # deepcopy to avoid modifying the original algo and set the data stats and data source config
                gen_algo = deepcopy(algo)
                gen_algo.set_data_stats(data_stats)
                gen_algo.set_data_source(data_src_cfg)
                name = f"{gen_algo.name}_{f_id}"
                gen_algo.export_to_disk(output_folder, name, fold=f_id)
                algo_to_pickle(gen_algo, template_path=algo.template_path)
                #self.history.append({name: gen_algo})  # track the previous, may create a persistent history
                self.history.append({AlgoKeys.ID: name, AlgoKeys.ALGO: gen_algo})  # track the previous, may create a persistent history - ESSENTIAL CHANGE TO WORK!!
                
        

class Finetuning:

    # Finetuning step on an unseen sample of images.

    # initialization
    def __init__(self, work_dir: str=".", dataroot: str = ".", json_file: str = ".", output_dir=None):

        self.Workdir=work_dir
        self.Dataroot=dataroot
        self.JSON_file=json_file
        if output_dir is None:
            self.Output_dir=join(self.Workdir, 'working_directory_finetuning')
        elif  isinstance(output_dir, str):
            self.Output_dir=output_dir
    
    def finetuning_run(self):

        dataroot = self.Dataroot
        work_dir = self.Output_dir

        # create working directory
        if not os.path.isdir(work_dir):
            os.makedirs(work_dir)

        algorithm_path=join(os.environ['ASCHOPLEXDIR'], 'DNN_models', 'algorithm_templates')

        da_output_yaml = join(work_dir, "datastats.yaml")
        data_src_cfg = join(work_dir, "data_src_cfg.yaml")

        if not os.path.isdir(dataroot):
            os.makedirs(dataroot)

        if not os.path.isdir(work_dir):
            os.makedirs(work_dir)


        # write to a json file
        datalist = self.JSON_file

        # 1. Analyze Dataset

        da = DataAnalyzer(datalist, dataroot, output_path=da_output_yaml)
        da.get_all_case_stats()

        data_src = {
            "modality": "MRI",
            "datalist": datalist,
            "dataroot": dataroot,
        }

        ConfigParser.export_config_file(data_src, data_src_cfg)

        # 2. Training

        bundle_generator = MyBundleGen(
            algo_path=work_dir, algo_template=algorithm_path, data_stats_filename=da_output_yaml, data_src_cfg_name=data_src_cfg
        )
        bundle_generator.generate(work_dir, num_fold=1)
        history = bundle_generator.get_history()
        ("DONE")
        export_bundle_algo_history(history)


        max_epochs = 100
        max_epochs = max(max_epochs, 2)

        train_param = {
            "CUDA_VISIBLE_DEVICES": [0],  # use only 1 gpu
            "num_iterations": 10000,
            "num_iterations_per_validation": 2 * max_epochs,
            "num_images_per_batch": 1,
            "num_epochs": max_epochs,
            "num_warmup_iterations": 2 * max_epochs,
        }

        for h in history: # [1::4] (added) to get only the algo_bunde objects
            for i, (_, algo) in enumerate(h.items()):
                if i % 2 == 1: 
                    algo.train(train_param)
                    print("DONE")
        
        return self.Output_dir




