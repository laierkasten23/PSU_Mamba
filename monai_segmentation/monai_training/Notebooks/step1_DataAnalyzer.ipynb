{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.0.1\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 1.13.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8271a193229fe4437026185e218d5b06f7c8ce69\n",
      "MONAI __file__: /var/data/student_home/lia/envs/base/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "Nibabel version: 5.2.0\n",
      "scikit-image version: 0.22.0\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.11.2\n",
      "gdown version: 4.7.3\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.64.1\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.6\n",
      "pandas version: 2.2.0\n",
      "einops version: 0.6.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.10.1\n",
      "pynrrd version: 1.0.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.apps.auto3dseg import DataAnalyzer\n",
    "from monai.config import print_config\n",
    "from monai.data import create_test_image_3d\n",
    "from monai.apps import download_and_extract\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datalist Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "join = os.path.join\n",
    "\n",
    "\n",
    "class WriteJSON:\n",
    "    \"\"\"\n",
    "    Class for writing .json files to run finetuning and/or the prediction of Choroid Plexus segmentations.\n",
    "\n",
    "    Args:\n",
    "        dataroot (str): Root directory of the data.\n",
    "        description (str, optional): Description of the dataset. Defaults to None.\n",
    "        work_dir (str): Working directory.\n",
    "        finetune (str): Flag indicating whether to perform finetuning. Defaults to \".\".\n",
    "        prediction (str): Flag indicating whether to perform prediction. Defaults to \".\".\n",
    "\n",
    "    Attributes:\n",
    "        dataroot (str): Root directory of the data.\n",
    "        description (str): Description of the dataset.\n",
    "        JSON_dir (str): Directory to store the JSON files.\n",
    "        finetune (str): Flag indicating whether to perform finetuning.\n",
    "        prediction (str): Flag indicating whether to perform prediction.\n",
    "        file (list): List to store the paths of the generated JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self, dataroot: str=\".\", description=None, work_dir: str=\".\", finetune: str=\".\", prediction: str=\".\"):\n",
    "        \"\"\"\n",
    "        Initializes the WriteJSON class.\n",
    "\n",
    "        Args:\n",
    "            dataroot (str): Root directory of the data.\n",
    "            description (str, optional): Description of the dataset. Defaults to None.\n",
    "            work_dir (str): Working directory.\n",
    "            finetune (str): Flag indicating whether to perform finetuning. Defaults to \".\".\n",
    "            prediction (str): Flag indicating whether to perform prediction. Defaults to \".\".\n",
    "        \"\"\"\n",
    "        self.dataroot=dataroot\n",
    "        if description is None:\n",
    "            self.description='Dataset for Choroid Plexus segmentation'\n",
    "        elif isinstance(description, str):\n",
    "            self.description=description\n",
    "        self.JSON_dir=work_dir\n",
    "        self.finetune=finetune\n",
    "        self.prediction=prediction\n",
    "        self.file=[]\n",
    "       \n",
    "    def write_json_file(self):\n",
    "        \n",
    "        # set data path\n",
    "        output_folder = join(self.JSON_dir, 'JSON_file')\n",
    "        \n",
    "        if not os.path.isdir(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        if self.finetune=='yes':\n",
    "        \n",
    "            train_id=True\n",
    "\n",
    "            if self.prediction=='yes':\n",
    "\n",
    "                test_id=True\n",
    "                test_ft=False\n",
    "\n",
    "            else: \n",
    "                # self.prediction=='no'\n",
    "                test_id=False\n",
    "                test_ft=False\n",
    "                \n",
    "            name_json=\"dataset_finetuning.json\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            train_id=False\n",
    "\n",
    "            if self.prediction=='yes':\n",
    "                \n",
    "                test_id=True\n",
    "                test_ft=False\n",
    "                name_json=\"dataset_prediction.json\"\n",
    "\n",
    "            else:\n",
    "                # self.prediction=='ft'\n",
    "                test_id=False\n",
    "                test_ft=True\n",
    "                name_json=\"dataset_finetuning.json\"\n",
    "\n",
    "\n",
    "        if train_id:\n",
    "            train_dir = join(self.dataroot, 'image_Tr')\n",
    "            label_dir = join(self.dataroot, 'label_Tr')\n",
    "            train_ids=[]\n",
    "            validation_ids=[]\n",
    "            label_train_ids = []\n",
    "            label_valid_ids=[]\n",
    "\n",
    "            filenames_image = os.listdir(train_dir)\n",
    "            filenames_image.sort()\n",
    "            filenames_label = os.listdir(label_dir)\n",
    "            filenames_label.sort()   \n",
    "\n",
    "            if len(filenames_image)!=len(filenames_label):\n",
    "                raise ValueError(\"The number of images and the number of labels is different. Please, check image_Tr and label_Tr folders.\")\n",
    "\n",
    "            # training\n",
    "            jj=math.ceil(len(filenames_image)/2)\n",
    "\n",
    "            for name in filenames_image[0:jj]:\n",
    "                if not(name.endswith('.nii') | name.endswith('.nii.gz')):\n",
    "                    raise ValueError(\"Data are not in the correct format. Please, provide images in .nii or .nii.gz Nifti format\")\n",
    "                image=join(train_dir, name)\n",
    "                train_ids.append(image)\n",
    "            \n",
    "            count=0\n",
    "            for name in filenames_label[0:jj]:\n",
    "                if not(name.endswith('.nii') | name.endswith('.nii.gz')):\n",
    "                    raise ValueError(\"Data are not in the correct format. Please, provide images in .nii or .nii.gz Nifti format\")\n",
    "                img_=os.path.basename(filenames_image[count]).replace('_image', '')\n",
    "                lab_=os.path.basename(name).replace('_seg', '')\n",
    "                if img_==lab_:\n",
    "                    label=join(label_dir, name)\n",
    "                    label_train_ids.append(label)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    raise ValueError(\"Subject identifier is not univoque. Please, pass correct data\")\n",
    "            \n",
    "            # validation\n",
    "\n",
    "            for name in filenames_image[jj:len(filenames_image)]:\n",
    "                if not(name.endswith('.nii') | name.endswith('.nii.gz')):\n",
    "                    raise ValueError(\"Data are not in the correct format. Please, provide images in .nii or .nii.gz Nifti format\")\n",
    "                image=join(train_dir, name)\n",
    "                validation_ids.append(image)\n",
    "            count=jj\n",
    "            for name in filenames_label[jj:len(filenames_image)]:\n",
    "                if not(name.endswith('.nii') | name.endswith('.nii.gz')):\n",
    "                    raise ValueError(\"Data are not in the correct format. Please, provide images in .nii or .nii.gz Nifti format\")\n",
    "                img_=os.path.basename(filenames_image[count]).replace('_image', '')\n",
    "                lab_=os.path.basename(name).replace('_seg', '')\n",
    "                if img_==lab_:\n",
    "                    label=join(label_dir, name)\n",
    "                    label_valid_ids.append(label)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    raise ValueError(\"Subject identifier is not univoque. Please, pass correct data\")\n",
    "\n",
    "\n",
    "        if test_id or test_ft:\n",
    "            #  testing\n",
    "            \n",
    "            test_dir=join(self.dataroot, 'image_Ts')\n",
    "            test_ids=[]\n",
    "            testnames = os.listdir(test_dir)\n",
    "            testnames.sort()\n",
    "\n",
    "            for test_name in testnames:\n",
    "                if not(test_name.endswith('.nii') | test_name.endswith('.nii.gz')):\n",
    "                    raise ValueError(\"Data are not in the correct format. Please, provide images in .nii or .nii.gz Nifti format\")\n",
    "                image=join(test_dir, test_name)\n",
    "                test_ids.append(image)\n",
    "\n",
    "\n",
    "        if (train_id and test_id) or (not(train_id) and test_id) or (train_id and not(test_id)):\n",
    "\n",
    "            # create json file - manually set\n",
    "\n",
    "            json_dict = OrderedDict()\n",
    "            json_dict['name'] = \"MRI Dataset - Choroid Plexus Segmentation\" \n",
    "            json_dict['description'] = self.description\n",
    "            json_dict['tensorImageSize'] = \"3D\"\n",
    "            json_dict['modality'] = {\n",
    "                \"0\": \"MR\"\n",
    "            }\n",
    "            \n",
    "            json_dict['labels'] = {\n",
    "                \"0\": \"background\",\n",
    "                \"1\": \"Choroid Plexus\"\n",
    "\n",
    "            }\n",
    "\n",
    "            if train_id and test_id:\n",
    "\n",
    "                json_dict['numTraining'] = len(train_ids)\n",
    "                json_dict['numValidation'] = len(validation_ids)\n",
    "                json_dict['numTest'] = len(test_ids)\n",
    "                json_dict['training'] = [{\"fold\": 0, \"image\": '%s' %i , \"label\": '%s' %j} for j, i in zip(label_train_ids, train_ids)]\n",
    "                json_dict['validation'] = [{\"image\": '%s' %i, \"label\": '%s' %j} for j,i in zip(label_valid_ids, validation_ids)]\n",
    "                json_dict['testing'] = [{\"image\": '%s' %i} for i in test_ids]\n",
    "\n",
    "            elif not(train_id) and test_id: \n",
    "\n",
    "                json_dict['numTest'] = len(test_ids)\n",
    "                json_dict['testing'] = [{\"image\": '%s' % i} for i in test_ids]\n",
    "\n",
    "            elif train_id and not(test_id):\n",
    "\n",
    "                json_dict['numTraining'] = len(train_ids)\n",
    "                json_dict['numValidation'] = len(validation_ids)\n",
    "                json_dict['training'] = [{\"fold\": 0, \"image\": '%s' %i , \"label\": '%s' %j} for j, i in zip(label_train_ids, train_ids)]\n",
    "                json_dict['validation'] = [{\"image\": '%s' %i, \"label\": '%s' %j} for j,i in zip(label_valid_ids, validation_ids)]\n",
    "\n",
    "            with open(join(output_folder, name_json), 'w') as f:\n",
    "                json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "        \n",
    "        elif test_ft:\n",
    "\n",
    "            # append lines to the .json files\n",
    "            with open(join(output_folder, name_json)) as f:\n",
    "                json_append=json.load(f)\n",
    "\n",
    "            json_append['numTest']=len(test_ids)\n",
    "\n",
    "            json_append['testing'] = [{\"image\": '%s' % i} for i in test_ids]\n",
    "\n",
    "            with open(join(output_folder, name_json), 'w') as f:\n",
    "                json.dump(json_append, f, indent=4, sort_keys=True)\n",
    "\n",
    "        self.file.append(join(output_folder, name_json))\n",
    "\n",
    "        return self.file[0] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
