{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Project-MONAI/tutorials/blob/main/auto3dseg/notebooks/auto_runner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "print(monai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from monai.apps.auto3dseg import AutoRunner, export_bundle_algo_history, import_bundle_algo_history\n",
    "from monai.utils.enums import AlgoKeys\n",
    "from monai.auto3dseg import algo_to_pickle\n",
    "from monai.config import print_config\n",
    "from monai.bundle.config_parser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/JSON_dir/train.json\"\n",
    "json_train_val_path = \"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/JSON_dir/train_val.json\"\n",
    "json_train_val_path = \"C:/Users/lia/Documents/Lia_Masterthesis\\phuse_thesis_2024\\monai_segmentation\\monai_training\\JSON_dir_experiments\\JSON_dir\\dataset_train_val.json\"\n",
    "json_train_val_path = \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/JSON_dir/dataset_train_val.json\"\n",
    "\n",
    "work_dir = os.path.join(\"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training\", 'working_dir_training_from_scratch_0327') \n",
    "work_dir = os.path.join(\"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training\", 'working_dir_training_from_scratch_0429') \n",
    "dataroot = \"/var/data/MONAI_Choroid_Plexus/dataset_monai_train_from_scratch\"\n",
    "\n",
    "work_dir = os.path.join('C:/Users/lia/Documents/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training', 'working_dir_training_from_scratch_0507')\n",
    "dataroot = \"C:/Users/lia/Documents/Lia_Masterthesis/data/dataset_aschoplex\"\n",
    "\n",
    "work_dir = os.path.join(\"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training\", 'working_directory_0509')\n",
    "dataroot = \"/home/linuxlia/Lia_Masterthesis/data/dataset_aschoplex\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the folder of the working directory exists!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = {\n",
    "    \"modality\": \"MRI\",\n",
    "    \"datalist\": json_train_val_path, #datalist, # give path to json file, it is not necessary to already read the json file\n",
    "    \"dataroot\": dataroot,\n",
    "}\n",
    "\n",
    "data_src_cfg = os.path.join(work_dir, \"data_src_cfg.yaml\")\n",
    "ConfigParser.export_config_file(data_src, data_src_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Auto3DSeg pipeline in a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-27 11:53:32,533 - INFO - AutoRunner using work directory ./work_dir\n",
      "2024-02-27 11:53:32,538 - INFO - Loading input config /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/working_dir_training_from_scratch/data_src_cfg.yaml\n",
      "2024-02-27 11:53:32,540 - INFO - Datalist was copied to work_dir: /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/train.json\n",
      "2024-02-27 11:53:32,542 - INFO - Setting num_fold 5 based on the input datalist /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/train.json.\n",
      "2024-02-27 11:53:32,573 - INFO - Using user defined command running prefix , will override other settings\n",
      "2024-02-27 11:53:32,575 - INFO - Running data analysis...\n",
      "2024-02-27 11:53:32,576 - INFO - Found 1 GPUs for data analyzing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:33<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-27 11:54:06,478 - INFO - Writing data stats to /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/datastats.yaml.\n",
      "2024-02-27 11:54:06,491 - INFO - Writing by-case data stats to /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/datastats_by_case.yaml, this may take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-27 11:54:06,692 - INFO - BundleGen from https://github.com/Project-MONAI/research-contributions/releases/download/algo_templates/249bf4b.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "algo_templates.tar.gz: 104kB [00:01, 67.7kB/s]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-27 11:54:08,271 - INFO - Downloaded: /tmp/tmp7soakx57/algo_templates.tar.gz\n",
      "2024-02-27 11:54:08,272 - INFO - Expected md5 is None, skip md5 check for file /tmp/tmp7soakx57/algo_templates.tar.gz.\n",
      "2024-02-27 11:54:08,274 - INFO - Writing into directory: /var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-27 11:54:08,599 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0\n",
      "2024-02-27 11:54:08,723 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_1\n",
      "2024-02-27 11:54:08,835 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_2\n",
      "2024-02-27 11:54:08,924 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_3\n",
      "2024-02-27 11:54:09,017 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_4\n",
      "2024-02-27 11:54:09,062 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/segresnet_0\n",
      "2024-02-27 11:54:09,117 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/segresnet_1\n",
      "2024-02-27 11:54:09,172 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/segresnet_2\n",
      "2024-02-27 11:54:09,229 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/segresnet_3\n",
      "2024-02-27 11:54:09,282 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/segresnet_4\n",
      "2024-02-27 11:54:09,314 - INFO - segresnet2d_0 is skipped! SegresNet2D is skipped due to median spacing of [1.0, 1.0, 1.0],  which means the dataset is not highly anisotropic, e.g. spacing[2] < 3*(spacing[0] + spacing[1])/2) .\n",
      "2024-02-27 11:54:09,336 - INFO - segresnet2d_1 is skipped! SegresNet2D is skipped due to median spacing of [1.0, 1.0, 1.0],  which means the dataset is not highly anisotropic, e.g. spacing[2] < 3*(spacing[0] + spacing[1])/2) .\n",
      "2024-02-27 11:54:09,360 - INFO - segresnet2d_2 is skipped! SegresNet2D is skipped due to median spacing of [1.0, 1.0, 1.0],  which means the dataset is not highly anisotropic, e.g. spacing[2] < 3*(spacing[0] + spacing[1])/2) .\n",
      "2024-02-27 11:54:09,382 - INFO - segresnet2d_3 is skipped! SegresNet2D is skipped due to median spacing of [1.0, 1.0, 1.0],  which means the dataset is not highly anisotropic, e.g. spacing[2] < 3*(spacing[0] + spacing[1])/2) .\n",
      "2024-02-27 11:54:09,407 - INFO - segresnet2d_4 is skipped! SegresNet2D is skipped due to median spacing of [1.0, 1.0, 1.0],  which means the dataset is not highly anisotropic, e.g. spacing[2] < 3*(spacing[0] + spacing[1])/2) .\n",
      "2024-02-27 11:54:09,483 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/swinunetr_0\n",
      "2024-02-27 11:54:09,596 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/swinunetr_1\n",
      "2024-02-27 11:54:09,688 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/swinunetr_2\n",
      "2024-02-27 11:54:09,784 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/swinunetr_3\n",
      "2024-02-27 11:54:09,872 - INFO - Generated:/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/swinunetr_4\n",
      "2024-02-27 11:54:10,066 - INFO - ['python', '/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/scripts/train.py', 'run', \"--config_file='/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/hyper_parameters.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/hyper_parameters_search.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/network.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/network_search.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/transforms_infer.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/transforms_train.yaml,/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/dints_0/configs/transforms_validate.yaml'\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dints_0 - training ...:   0%|          | 0/232 [00:00<?, ?round/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m runner \u001b[38;5;241m=\u001b[39m AutoRunner(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata_src_cfg)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/base/lib/python3.10/site-packages/monai/apps/auto3dseg/auto_runner.py:806\u001b[0m, in \u001b[0;36mAutoRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhpo:\n\u001b[0;32m--> 806\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_algo_in_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_algo_in_nni(history)\n",
      "File \u001b[0;32m~/envs/base/lib/python3.10/site-packages/monai/apps/auto3dseg/auto_runner.py:658\u001b[0m, in \u001b[0;36mAutoRunner._train_algo_in_sequence\u001b[0;34m(self, history)\u001b[0m\n\u001b[1;32m    656\u001b[0m algo \u001b[38;5;241m=\u001b[39m algo_dict[AlgoKeys\u001b[38;5;241m.\u001b[39mALGO]\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_option(algo\u001b[38;5;241m.\u001b[39mtrain, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_setting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 658\u001b[0m     \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_setting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     algo\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_params)\n",
      "File \u001b[0;32m~/phuse_thesis_2024/monai_segmentation/monai_training/work_dir/algorithm_templates/dints/scripts/algo.py:490\u001b[0m, in \u001b[0;36mDintsAlgo.train\u001b[0;34m(self, train_params, device_setting, search)\u001b[0m\n\u001b[1;32m    488\u001b[0m cmd, devices_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_cmd(dints_train_params)\n\u001b[1;32m    489\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS=1 \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cmd\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/base/lib/python3.10/site-packages/monai/apps/auto3dseg/bundle_gen.py:255\u001b[0m, in \u001b[0;36mBundleAlgo._run_cmd\u001b[0;34m(self, cmd, devices_info)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _run_cmd_torchrun(\n\u001b[1;32m    252\u001b[0m         cmd, nnodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, nproc_per_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_setting[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m], env\u001b[38;5;241m=\u001b[39mps_environ, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_cmd_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mps_environ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/base/lib/python3.10/site-packages/monai/utils/misc.py:874\u001b[0m, in \u001b[0;36mrun_cmd\u001b[0;34m(cmd_list, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m     monai\u001b[38;5;241m.\u001b[39mapps\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_cmd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    505\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1144\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1207\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1941\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1941\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1899\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1899\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner = AutoRunner(input=data_src_cfg) \n",
    "''' \n",
    "Running AutoRunner will perform the following steps:\n",
    "1. Load the data source and the algorithm configuration.\n",
    "2. Load the data and preprocess it.\n",
    "3. Download the pre-trained model from the url specified in the algorithm configuration.\n",
    "4. Create folders for each algorithm. \n",
    "'''\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:30:17,089 - INFO - AutoRunner using work directory /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509\n",
      "2024-05-09 17:30:17,096 - INFO - Loading input config /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/data_src_cfg.yaml\n",
      "2024-05-09 17:30:17,098 - INFO - Datalist was copied to work_dir: /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/dataset_train_val.json\n",
      "2024-05-09 17:30:17,099 - INFO - Setting num_fold 5 based on the input datalist /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/dataset_train_val.json.\n",
      "2024-05-09 17:30:17,122 - INFO - Using user defined command running prefix , will override other settings\n",
      "2024-05-09 17:30:17,123 - INFO - Running data analysis...\n",
      "2024-05-09 17:30:17,123 - INFO - Found 1 GPUs for data analyzing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:04<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:30:21,901 - INFO - Data spacing is not completely uniform. MONAI transforms may provide unexpected result\n",
      "2024-05-09 17:30:21,902 - INFO - Writing data stats to /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/datastats.yaml.\n",
      "2024-05-09 17:30:21,916 - INFO - Writing by-case data stats to /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/datastats_by_case.yaml, this may take a while.\n",
      "2024-05-09 17:30:21,974 - INFO - BundleGen from directory /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\n",
      "2024-05-09 17:30:21,983 - INFO - Copying template: DynUnet_128_Dice_2 -- {'_target_': 'DynUnet_128_Dice_2.scripts.algo.Dynunet_128_dice_2Algo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 17:30:21,984 - INFO - Copying template: SwinUnetr -- {'_target_': 'SwinUnetr.scripts.algo.SwinunetrAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 17:30:21,985 - INFO - Copying template: UNETR -- {'_target_': 'UNETR.scripts.algo.UnetrAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 17:30:21,985 - INFO - Copying template: UNET -- {'_target_': 'UNET.scripts.algo.UnetAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 17:30:21,986 - INFO - Copying template: segresnet -- {'_target_': 'segresnet.scripts.algo.SegresnetAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 17:30:21,986 - INFO - Copying template: dints -- {'_target_': 'dints.scripts.algo.DintsAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:30:22,135 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0\n",
      "2024-05-09 17:30:22,196 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1\n",
      "2024-05-09 17:30:22,253 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2\n",
      "2024-05-09 17:30:22,308 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3\n",
      "2024-05-09 17:30:22,371 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4\n",
      "2024-05-09 17:30:22,430 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/configs/transforms_validate.yaml'\"]\n",
      "output_classes:  2\n",
      "l---------og_output_file:  /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/model/training.log\n",
      "CONFIG:  {'version': 1, 'disable_existing_loggers': False, 'formatters': {'monai_default': {'format': '%(asctime)s - %(levelname)s - %(message)s'}}, 'loggers': {'monai.apps.auto3dseg.auto_runner': {'handlers': ['file', 'console'], 'level': 'DEBUG', 'propagate': False}}, 'filters': {'rank_filter': {'()': <class 'monai.utils.dist.RankFilter'>}}, 'handlers': {'file': {'class': 'logging.FileHandler', 'filename': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_0/model/training.log', 'mode': 'a', 'level': 'DEBUG', 'formatter': 'monai_default', 'filters': ['rank_filter']}, 'console': {'class': 'logging.StreamHandler', 'level': 'INFO', 'formatter': 'monai_default', 'filters': ['rank_filter']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 17:30:26 INFO mlflow.tracking.fluent: Experiment with name 'Auto3DSeg' does not exist. Creating a new experiment.\n",
      "SwinUnetr_0 - training ...:   0%|          | 0/4000 [00:00<?, ?round/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [24.40380096435547, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 1/4000 [00:22<25:18:37, 22.79s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [18.474842071533203, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 2/4000 [00:43<23:57:27, 21.57s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [15.40932846069336, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 3/4000 [01:04<23:25:23, 21.10s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.127894401550293, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 4/4000 [01:24<23:08:11, 20.84s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [13.349431991577148, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 5/4000 [01:44<22:56:58, 20.68s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [12.784697532653809, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 6/4000 [02:06<23:09:39, 20.88s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [12.047528266906738, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 7/4000 [02:27<23:18:12, 21.01s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.140411376953125, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 8/4000 [02:48<23:13:00, 20.94s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [10.240039825439453, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 9/4000 [03:08<23:04:41, 20.82s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [10.290338516235352, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 10/4000 [03:29<22:53:58, 20.66s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.543790817260742, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 11/4000 [03:49<22:54:46, 20.68s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.065690994262695, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 12/4000 [04:10<22:53:43, 20.67s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.591263771057129, 20.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_0 - training ...:   0%|          | 12/4000 [04:31<25:02:08, 22.60s/round]\n",
      "2024-05-09 17:35:36,780 - WARNING - SwinUnetr_0 - training: finished with early stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:35:38,101 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/configs/transforms_validate.yaml'\"]\n",
      "output_classes:  2\n",
      "l---------og_output_file:  /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/model/training.log\n",
      "CONFIG:  {'version': 1, 'disable_existing_loggers': False, 'formatters': {'monai_default': {'format': '%(asctime)s - %(levelname)s - %(message)s'}}, 'loggers': {'monai.apps.auto3dseg.auto_runner': {'handlers': ['file', 'console'], 'level': 'DEBUG', 'propagate': False}}, 'filters': {'rank_filter': {'()': <class 'monai.utils.dist.RankFilter'>}}, 'handlers': {'file': {'class': 'logging.FileHandler', 'filename': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_1/model/training.log', 'mode': 'a', 'level': 'DEBUG', 'formatter': 'monai_default', 'filters': ['rank_filter']}, 'console': {'class': 'logging.StreamHandler', 'level': 'INFO', 'formatter': 'monai_default', 'filters': ['rank_filter']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 17:35:42 INFO mlflow.tracking.fluent: Experiment with name 'Auto3DSeg' does not exist. Creating a new experiment.\n",
      "SwinUnetr_1 - training ...:   0%|          | 0/4000 [00:00<?, ?round/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [39.1593017578125, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 1/4000 [00:27<30:13:18, 27.21s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [28.338525772094727, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 2/4000 [00:50<27:54:31, 25.13s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [24.514047622680664, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 3/4000 [01:14<27:06:43, 24.42s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [22.69473648071289, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 4/4000 [01:38<27:08:39, 24.45s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [21.39238739013672, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 5/4000 [02:03<27:00:14, 24.33s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [20.015716552734375, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 6/4000 [02:27<26:55:48, 24.27s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [18.707504272460938, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 7/4000 [02:51<26:50:49, 24.20s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [15.921496391296387, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 8/4000 [03:15<26:47:12, 24.16s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.743086814880371, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 9/4000 [03:39<26:43:12, 24.10s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [13.299888610839844, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 10/4000 [04:03<26:46:38, 24.16s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.339445114135742, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 11/4000 [04:28<26:50:47, 24.23s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.866501808166504, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 12/4000 [04:52<26:50:49, 24.24s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.709351539611816, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 13/4000 [05:16<26:52:40, 24.27s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.330277442932129, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 14/4000 [05:40<26:53:25, 24.29s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.174161911010742, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 15/4000 [06:04<26:41:16, 24.11s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.72783088684082, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 16/4000 [06:28<26:32:09, 23.98s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.878812789916992, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 17/4000 [06:52<26:35:38, 24.04s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.599677085876465, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 18/4000 [07:16<26:41:04, 24.12s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.841161727905273, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 19/4000 [07:40<26:41:01, 24.13s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.243005752563477, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   0%|          | 20/4000 [08:04<26:38:55, 24.10s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.585814952850342, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   1%|          | 21/4000 [08:28<26:34:33, 24.04s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.131990432739258, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   1%|          | 22/4000 [08:52<26:27:14, 23.94s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.938624382019043, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_1 - training ...:   1%|          | 22/4000 [09:16<27:57:21, 25.30s/round]\n",
      "2024-05-09 17:45:09,795 - WARNING - SwinUnetr_1 - training: finished with early stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:45:11,051 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/configs/transforms_validate.yaml'\"]\n",
      "output_classes:  2\n",
      "l---------og_output_file:  /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/model/training.log\n",
      "CONFIG:  {'version': 1, 'disable_existing_loggers': False, 'formatters': {'monai_default': {'format': '%(asctime)s - %(levelname)s - %(message)s'}}, 'loggers': {'monai.apps.auto3dseg.auto_runner': {'handlers': ['file', 'console'], 'level': 'DEBUG', 'propagate': False}}, 'filters': {'rank_filter': {'()': <class 'monai.utils.dist.RankFilter'>}}, 'handlers': {'file': {'class': 'logging.FileHandler', 'filename': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_2/model/training.log', 'mode': 'a', 'level': 'DEBUG', 'formatter': 'monai_default', 'filters': ['rank_filter']}, 'console': {'class': 'logging.StreamHandler', 'level': 'INFO', 'formatter': 'monai_default', 'filters': ['rank_filter']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 17:45:15 INFO mlflow.tracking.fluent: Experiment with name 'Auto3DSeg' does not exist. Creating a new experiment.\n",
      "SwinUnetr_2 - training ...:   0%|          | 0/4000 [00:00<?, ?round/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [39.145362854003906, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 1/4000 [00:28<31:22:43, 28.25s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [28.123186111450195, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 2/4000 [00:53<29:39:50, 26.71s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [24.360939025878906, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 3/4000 [01:19<29:03:31, 26.17s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [22.623533248901367, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 4/4000 [01:45<29:01:35, 26.15s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [21.157209396362305, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 5/4000 [02:11<29:02:58, 26.18s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [19.778886795043945, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 6/4000 [02:37<28:52:46, 26.03s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [18.57914924621582, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 7/4000 [03:03<28:56:09, 26.09s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [16.088062286376953, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 8/4000 [03:29<28:59:59, 26.15s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.564323425292969, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 9/4000 [03:55<28:44:36, 25.93s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [13.739530563354492, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 10/4000 [04:21<28:38:02, 25.84s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [13.825489044189453, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 11/4000 [04:47<28:43:41, 25.93s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.591214179992676, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 12/4000 [05:12<28:35:49, 25.81s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [10.446120262145996, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 13/4000 [05:38<28:38:02, 25.85s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.02019214630127, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 14/4000 [06:04<28:38:34, 25.87s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.204886436462402, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 15/4000 [06:30<28:35:32, 25.83s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.38249397277832, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 16/4000 [06:55<28:30:16, 25.76s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.088924407958984, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 17/4000 [07:22<28:37:27, 25.87s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.271013259887695, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 18/4000 [07:47<28:38:06, 25.89s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.773340225219727, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 19/4000 [08:14<28:40:54, 25.94s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.18514633178711, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   0%|          | 20/4000 [08:39<28:36:57, 25.88s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [6.475800514221191, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 21/4000 [09:05<28:31:13, 25.80s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.352521896362305, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 22/4000 [09:31<28:40:01, 25.94s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.362558364868164, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 23/4000 [09:57<28:38:43, 25.93s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.8289594650268555, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 24/4000 [10:23<28:38:01, 25.93s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.696109294891357, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 25/4000 [10:49<28:35:09, 25.89s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.792977809906006, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 26/4000 [11:15<28:38:50, 25.95s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.226587772369385, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_2 - training ...:   1%|          | 26/4000 [11:40<29:45:13, 26.95s/round]\n",
      "2024-05-09 17:57:06,838 - WARNING - SwinUnetr_2 - training: finished with early stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 17:57:08,160 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/configs/transforms_validate.yaml'\"]\n",
      "output_classes:  2\n",
      "l---------og_output_file:  /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/model/training.log\n",
      "CONFIG:  {'version': 1, 'disable_existing_loggers': False, 'formatters': {'monai_default': {'format': '%(asctime)s - %(levelname)s - %(message)s'}}, 'loggers': {'monai.apps.auto3dseg.auto_runner': {'handlers': ['file', 'console'], 'level': 'DEBUG', 'propagate': False}}, 'filters': {'rank_filter': {'()': <class 'monai.utils.dist.RankFilter'>}}, 'handlers': {'file': {'class': 'logging.FileHandler', 'filename': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_3/model/training.log', 'mode': 'a', 'level': 'DEBUG', 'formatter': 'monai_default', 'filters': ['rank_filter']}, 'console': {'class': 'logging.StreamHandler', 'level': 'INFO', 'formatter': 'monai_default', 'filters': ['rank_filter']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 17:57:12 INFO mlflow.tracking.fluent: Experiment with name 'Auto3DSeg' does not exist. Creating a new experiment.\n",
      "SwinUnetr_3 - training ...:   0%|          | 0/4000 [00:00<?, ?round/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [39.25251770019531, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 1/4000 [00:26<29:59:05, 26.99s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [28.220243453979492, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 2/4000 [00:51<28:08:55, 25.35s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [24.423852920532227, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 3/4000 [01:15<27:22:47, 24.66s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [22.689218521118164, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 4/4000 [01:39<27:22:49, 24.67s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [21.387815475463867, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 5/4000 [02:04<27:28:18, 24.76s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [20.062286376953125, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 6/4000 [02:28<27:14:50, 24.56s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [18.894723892211914, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 7/4000 [02:53<27:19:29, 24.64s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [16.38339614868164, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 8/4000 [03:17<27:08:47, 24.48s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.97012710571289, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 9/4000 [03:41<27:01:29, 24.38s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [15.260869026184082, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 10/4000 [04:06<27:09:13, 24.50s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.2649564743042, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 11/4000 [04:30<27:00:56, 24.38s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [12.599282264709473, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 12/4000 [04:55<27:12:07, 24.56s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.994333267211914, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 13/4000 [05:20<27:07:54, 24.50s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.964024543762207, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 14/4000 [05:44<27:10:35, 24.54s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.565171241760254, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 15/4000 [06:08<27:01:17, 24.41s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [10.929931640625, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 16/4000 [06:33<27:02:26, 24.43s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.63636302947998, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 17/4000 [06:57<26:47:43, 24.22s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [10.620292663574219, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 18/4000 [07:21<26:54:30, 24.33s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.889449119567871, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 19/4000 [07:45<26:50:07, 24.27s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.721264839172363, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   0%|          | 20/4000 [08:10<26:50:02, 24.27s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.51356315612793, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 21/4000 [08:34<26:55:34, 24.36s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.362622261047363, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 22/4000 [08:59<26:59:43, 24.43s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.6371660232543945, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 23/4000 [09:24<27:11:45, 24.62s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.413860321044922, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 24/4000 [09:48<26:59:00, 24.43s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.206835746765137, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 25/4000 [10:12<26:51:50, 24.33s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.759782791137695, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 26/4000 [10:36<26:53:20, 24.36s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.050811767578125, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 27/4000 [11:01<26:53:32, 24.37s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.245973587036133, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 28/4000 [11:25<26:49:53, 24.32s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.756961822509766, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 29/4000 [11:49<26:48:11, 24.30s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.276545524597168, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 30/4000 [12:13<26:48:20, 24.31s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.619108200073242, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 31/4000 [12:38<26:45:06, 24.26s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [6.75347375869751, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 32/4000 [13:02<26:40:59, 24.21s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.312464714050293, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 33/4000 [13:26<26:48:21, 24.33s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [8.549460411071777, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 34/4000 [13:50<26:39:55, 24.20s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.845862865447998, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 35/4000 [14:15<26:41:15, 24.23s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [7.860705375671387, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 36/4000 [14:38<26:33:15, 24.12s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [5.708597660064697, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 37/4000 [15:02<26:27:58, 24.04s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [5.472814559936523, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_3 - training ...:   1%|          | 37/4000 [15:26<27:34:11, 25.04s/round]\n",
      "2024-05-09 18:12:49,564 - WARNING - SwinUnetr_3 - training: finished with early stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 18:12:50,980 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/configs/transforms_validate.yaml'\"]\n",
      "output_classes:  2\n",
      "l---------og_output_file:  /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/model/training.log\n",
      "CONFIG:  {'version': 1, 'disable_existing_loggers': False, 'formatters': {'monai_default': {'format': '%(asctime)s - %(levelname)s - %(message)s'}}, 'loggers': {'monai.apps.auto3dseg.auto_runner': {'handlers': ['file', 'console'], 'level': 'DEBUG', 'propagate': False}}, 'filters': {'rank_filter': {'()': <class 'monai.utils.dist.RankFilter'>}}, 'handlers': {'file': {'class': 'logging.FileHandler', 'filename': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/SwinUnetr_4/model/training.log', 'mode': 'a', 'level': 'DEBUG', 'formatter': 'monai_default', 'filters': ['rank_filter']}, 'console': {'class': 'logging.StreamHandler', 'level': 'INFO', 'formatter': 'monai_default', 'filters': ['rank_filter']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 18:12:55 INFO mlflow.tracking.fluent: Experiment with name 'Auto3DSeg' does not exist. Creating a new experiment.\n",
      "SwinUnetr_4 - training ...:   0%|          | 0/4000 [00:00<?, ?round/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.3350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.2079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.1187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(1.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [39.22828674316406, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 1/4000 [00:28<31:30:18, 28.36s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.9281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.8179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [28.210073471069336, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 2/4000 [00:54<29:44:12, 26.78s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.7224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [24.4791202545166, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 3/4000 [01:19<28:52:59, 26.01s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [22.8531551361084, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 4/4000 [01:44<28:44:22, 25.89s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [21.4057559967041, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 5/4000 [02:10<28:44:21, 25.90s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [20.024866104125977, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 6/4000 [02:36<28:40:34, 25.85s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.6048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [19.4409122467041, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 7/4000 [03:02<28:38:46, 25.83s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [16.52598762512207, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 8/4000 [03:28<28:47:53, 25.97s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [15.387123107910156, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 9/4000 [03:54<28:36:34, 25.81s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [14.705741882324219, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 10/4000 [04:19<28:36:46, 25.82s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [13.901045799255371, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 11/4000 [04:45<28:33:28, 25.77s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [12.63676643371582, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 12/4000 [05:11<28:30:10, 25.73s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.161049842834473, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 13/4000 [05:36<28:28:53, 25.72s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.4032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [11.107451438903809, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 14/4000 [06:02<28:26:22, 25.69s/round]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.5165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.3878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "num_patches_per_iter:  1\n",
      "check wheter its entering the for loop  1\n",
      "In swinunetr train, using amp\n",
      "Current loss:  tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "In swinunetr train, loss_torch:  [9.855938911437988, 35.0]\n",
      "trying to compute loss_torch_epoch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SwinUnetr_4 - training ...:   0%|          | 14/4000 [06:28<30:42:51, 27.74s/round]\n",
      "2024-05-09 18:19:34,343 - WARNING - SwinUnetr_4 - training: finished with early stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 18:19:35,656 - INFO - Ensembling using single GPU!\n",
      "2024-05-09 18:19:35,657 - INFO - The output_dir is not specified. /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/ensemble_output will be used to save ensemble predictions.\n",
      "2024-05-09 18:19:35,659 - INFO - Directory /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/ensemble_output is created to save ensemble predictions\n",
      "2024-05-09 18:19:35,776 - INFO - Datalist file has no testing key - testing. No data for inference is specified\n",
      "2024-05-09 18:19:35,777 - INFO - No testing files for inference is provided. Ensembler ending.\n",
      "2024-05-09 18:19:35,778 - INFO - Auto3Dseg pipeline is completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nRunning AutoRunner will perform the following steps:\\n1. Load the data source and the algorithm configuration.\\n2. Load the data and preprocess it.\\n3. Download the pre-trained model from the url specified in the algorithm configuration.\\n4. Create folders for each algorithm. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir = work_dir\n",
    "input = data_src_cfg\n",
    "#algos = \"UNET\"\n",
    "algos = \"SwinUnetr\"\n",
    "#algos = \"UNETR\"\n",
    "templates_path_or_url = \"C:/Users/lia/Documents/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "#templates_path_or_url = \"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "templates_path_or_url = \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "runner = AutoRunner(work_dir=work_dir, input=input, algos=algos, templates_path_or_url=templates_path_or_url)\n",
    "runner.run()\n",
    "\n",
    "#runner = AutoRunner(input=data_src_cfg, algo_gen=False) \n",
    "''' \n",
    "Running AutoRunner will perform the following steps:\n",
    "1. Load the data source and the algorithm configuration.\n",
    "2. Load the data and preprocess it.\n",
    "3. Download the pre-trained model from the url specified in the algorithm configuration.\n",
    "4. Create folders for each algorithm. \n",
    "'''\n",
    "#runner.run()\n",
    "\n",
    "# this also does not work because there are no algorithms in the working directory. This means, we need to create the algorithms first and then run the AutoRunner again. \n",
    "# Therefore, we need to modify the BundleGen and AlgoGen scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 14:46:54,988 - INFO - AutoRunner using work directory /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509\n",
      "2024-05-09 14:46:54,995 - INFO - Loading input config /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/data_src_cfg.yaml\n",
      "2024-05-09 14:46:54,998 - INFO - Datalist was copied to work_dir: /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/dataset_train_val.json\n",
      "2024-05-09 14:46:55,000 - INFO - Setting num_fold 5 based on the input datalist /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/dataset_train_val.json.\n",
      "2024-05-09 14:46:55,067 - INFO - Using user defined command running prefix , will override other settings\n",
      "2024-05-09 14:46:55,068 - INFO - Running data analysis...\n",
      "2024-05-09 14:46:55,069 - INFO - Found 1 GPUs for data analyzing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:04<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 14:47:00,481 - INFO - Data spacing is not completely uniform. MONAI transforms may provide unexpected result\n",
      "2024-05-09 14:47:00,482 - INFO - Writing data stats to /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/datastats.yaml.\n",
      "2024-05-09 14:47:00,492 - INFO - Writing by-case data stats to /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/datastats_by_case.yaml, this may take a while.\n",
      "2024-05-09 14:47:00,553 - INFO - BundleGen from directory /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\n",
      "2024-05-09 14:47:00,575 - INFO - Copying template: DynUnet_128_Dice_2 -- {'_target_': 'DynUnet_128_Dice_2.scripts.algo.Dynunet_128_dice_2Algo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 14:47:00,577 - INFO - Copying template: SwinUnetr -- {'_target_': 'SwinUnetr.scripts.algo.SwinunetrAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 14:47:00,578 - INFO - Copying template: UNETR -- {'_target_': 'UNETR.scripts.algo.UnetrAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 14:47:00,578 - INFO - Copying template: UNET -- {'_target_': 'UNET.scripts.algo.UnetAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 14:47:00,579 - INFO - Copying template: segresnet -- {'_target_': 'segresnet.scripts.algo.SegresnetAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n",
      "2024-05-09 14:47:00,580 - INFO - Copying template: dints -- {'_target_': 'dints.scripts.algo.DintsAlgo', 'template_path': '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/algorithm_templates'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- UnetAlgo ----------------------\n",
      "---------------------- fill_records done ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "2024-05-09 14:47:00,664 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0\n",
      "---------------------- UnetAlgo ----------------------\n",
      "---------------------- fill_records done ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "2024-05-09 14:47:00,721 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_1\n",
      "---------------------- UnetAlgo ----------------------\n",
      "---------------------- fill_records done ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "2024-05-09 14:47:00,779 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_2\n",
      "---------------------- UnetAlgo ----------------------\n",
      "---------------------- fill_records done ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "2024-05-09 14:47:00,849 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_3\n",
      "---------------------- UnetAlgo ----------------------\n",
      "---------------------- fill_records done ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "---------------------- parsed stuff  ----------------------\n",
      "2024-05-09 14:47:00,904 - INFO - Generated:/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_4\n",
      "2024-05-09 14:47:00,955 - INFO - ['python', '/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/scripts/train.py', 'run', \"--config_file='/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/configs/hyper_parameters.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/configs/network.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/configs/transforms_infer.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/configs/transforms_train.yaml,/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/configs/transforms_validate.yaml'\"]\n",
      "IN MAIN OF PRINT of UNET train.py\n",
      "IN RUN OF PRINT\n",
      "amp True\n",
      "ckpt_path /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/UNET_0/model\n",
      "data_file_base_dir /home/linuxlia/Lia_Masterthesis/data/dataset_aschoplex\n",
      "data_list_file_path /home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/monai_training/working_directory_0509/dataset_train_val.json\n",
      "determ False\n",
      "num_images_per_batch 2\n",
      "num_iterations 20000\n",
      "patch_size_valid [128, 128, 128]\n",
      "PARSING SUCCESSFUL!\n",
      "[info] number of GPUs: 1\n",
      "[info] world_size: 1\n",
      "train_files: 8\n",
      "val_files: 2\n",
      "num_epochs 5000\n",
      "num_epochs_per_validation 125\n",
      "[info] training from scratch\n",
      "[info] amp enabled\n",
      "----------\n",
      "epoch 1/5000\n",
      "learning rate is set to 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxlia/envs/monai13/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:47:07] 1/4, train_loss: 1.3439\n",
      "[2024-05-09 14:47:07] 2/4, train_loss: 1.3369\n",
      "[2024-05-09 14:47:07] 3/4, train_loss: 1.3226\n",
      "[2024-05-09 14:47:07] 4/4, train_loss: 1.3194\n",
      "epoch 1 average loss: 1.3307, best mean dice: -1.0000 at epoch -1\n",
      "----------\n",
      "epoch 2/5000\n",
      "learning rate is set to 0.0001\n",
      "[2024-05-09 14:47:09] 1/4, train_loss: 1.3125\n",
      "[2024-05-09 14:47:09] 2/4, train_loss: 1.2959\n",
      "[2024-05-09 14:47:10] 3/4, train_loss: 1.2913\n",
      "[2024-05-09 14:47:10] 4/4, train_loss: 1.2861\n",
      "epoch 2 average loss: 1.2964, best mean dice: -1.0000 at epoch -1\n",
      "----------\n",
      "epoch 3/5000\n",
      "learning rate is set to 0.0001\n",
      "[2024-05-09 14:47:12] 1/4, train_loss: 1.2781\n",
      "[2024-05-09 14:47:12] 2/4, train_loss: 1.2703\n",
      "[2024-05-09 14:47:12] 3/4, train_loss: 1.2588\n",
      "[2024-05-09 14:47:12] 4/4, train_loss: 1.2573\n",
      "epoch 3 average loss: 1.2662, best mean dice: -1.0000 at epoch -1\n",
      "----------\n",
      "epoch 4/5000\n",
      "learning rate is set to 0.0001\n",
      "[2024-05-09 14:47:14] 1/4, train_loss: 1.2498\n",
      "[2024-05-09 14:47:14] 2/4, train_loss: 1.2398\n",
      "[2024-05-09 14:47:15] 3/4, train_loss: 1.2355\n",
      "[2024-05-09 14:47:15] 4/4, train_loss: 1.2294\n",
      "epoch 4 average loss: 1.2386, best mean dice: -1.0000 at epoch -1\n",
      "----------\n",
      "epoch 5/5000\n",
      "learning rate is set to 0.0001\n",
      "[2024-05-09 14:47:17] 1/4, train_loss: 1.2231\n",
      "[2024-05-09 14:47:17] 2/4, train_loss: 1.2159\n",
      "[2024-05-09 14:47:18] 3/4, train_loss: 1.2114\n",
      "[2024-05-09 14:47:18] 4/4, train_loss: 1.2051\n",
      "epoch 5 average loss: 1.2139, best mean dice: -1.0000 at epoch -1\n",
      "----------\n",
      "epoch 6/5000\n",
      "learning rate is set to 0.0001\n",
      "[2024-05-09 14:47:20] 1/4, train_loss: 1.1942\n",
      "[2024-05-09 14:47:20] 2/4, train_loss: 1.1935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m templates_path_or_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m runner \u001b[38;5;241m=\u001b[39m AutoRunner(work_dir\u001b[38;5;241m=\u001b[39mwork_dir, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, algos\u001b[38;5;241m=\u001b[39malgos, templates_path_or_url\u001b[38;5;241m=\u001b[39mtemplates_path_or_url)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/monai13/lib/python3.9/site-packages/monai/apps/auto3dseg/auto_runner.py:806\u001b[0m, in \u001b[0;36mAutoRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhpo:\n\u001b[0;32m--> 806\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_algo_in_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_algo_in_nni(history)\n",
      "File \u001b[0;32m~/envs/monai13/lib/python3.9/site-packages/monai/apps/auto3dseg/auto_runner.py:658\u001b[0m, in \u001b[0;36mAutoRunner._train_algo_in_sequence\u001b[0;34m(self, history)\u001b[0m\n\u001b[1;32m    656\u001b[0m algo \u001b[38;5;241m=\u001b[39m algo_dict[AlgoKeys\u001b[38;5;241m.\u001b[39mALGO]\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_option(algo\u001b[38;5;241m.\u001b[39mtrain, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_setting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 658\u001b[0m     \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_setting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     algo\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_params)\n",
      "File \u001b[0;32m~/envs/monai13/lib/python3.9/site-packages/monai/apps/auto3dseg/bundle_gen.py:278\u001b[0m, in \u001b[0;36mBundleAlgo.train\u001b[0;34m(self, train_params, device_setting)\u001b[0m\n\u001b[1;32m    275\u001b[0m     train_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m cmd, _unused_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_cmd(train_params)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/monai13/lib/python3.9/site-packages/monai/apps/auto3dseg/bundle_gen.py:255\u001b[0m, in \u001b[0;36mBundleAlgo._run_cmd\u001b[0;34m(self, cmd, devices_info)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _run_cmd_torchrun(\n\u001b[1;32m    252\u001b[0m         cmd, nnodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, nproc_per_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_setting[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m], env\u001b[38;5;241m=\u001b[39mps_environ, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_cmd_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mps_environ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/monai13/lib/python3.9/site-packages/monai/utils/misc.py:874\u001b[0m, in \u001b[0;36mrun_cmd\u001b[0;34m(cmd_list, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m     monai\u001b[38;5;241m.\u001b[39mapps\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_cmd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug:\n",
      "File \u001b[0;32m/usr/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.9/subprocess.py:1126\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/subprocess.py:1933\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1933\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.9/subprocess.py:1891\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1891\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "work_dir = work_dir\n",
    "input = data_src_cfg\n",
    "algos = \"UNET\"\n",
    "#algos = \"SwinUnetr\"\n",
    "#algos = \"UNETR\"\n",
    "templates_path_or_url = \"C:/Users/lia/Documents/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "#templates_path_or_url = \"/var/data/student_home/lia/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "templates_path_or_url = \"/home/linuxlia/Lia_Masterthesis/phuse_thesis_2024/monai_segmentation/DNN_models/algorithm_templates_yaml/\"\n",
    "\n",
    "runner = AutoRunner(work_dir=work_dir, input=input, algos=algos, templates_path_or_url=templates_path_or_url)\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
