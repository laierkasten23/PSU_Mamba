{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create json file for from scratch training with our data (08.02.2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteTrainJSON:\n",
    "    \"\"\"\n",
    "    Class for writing .json files to run from training from scratch, finetuning and/or the prediction of Choroid Plexus segmentations.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataroot: str=\".\", description=None, work_dir: str=\".\", train: str=\".\"):\n",
    "        \"\"\"\n",
    "        Initializes the class with the given parameters.\n",
    "\n",
    "        :param dataroot: The path to the data directory. (/var/data/MONAI_Choroid_Plexus/dataset_monai)\n",
    "        :param description: The description of the experiment.\n",
    "        :param work_dir: The working directory. (/var/data/student_home/lia/thesis/monai_segmentation/monai_training)\n",
    "        :param train: The path to the train directory.\n",
    "        \"\"\"\n",
    "        self.dataroot = dataroot\n",
    "        if description is None:\n",
    "            self.description='Dataset for Choroid Plexus segmentation'\n",
    "        elif isinstance(description, str):\n",
    "            self.description=description\n",
    "        self.work_dir = work_dir\n",
    "        self.train = train\n",
    "        self.file=[]\n",
    "\n",
    "    def write_train_json(self, json_filename: str=\"train.json\"):\n",
    "\n",
    "        # Set path to output file\n",
    "        output_folder = os.path.join(self.work_dir, 'JSON_dir')\n",
    "\n",
    "        # Create output folder if it does not exist\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        image_dir = self.dataroot\n",
    "        label_dir = os.path.join(self.dataroot, 'labels', 'final')\n",
    "\n",
    "        filenames_image = os.listdir(image_dir)\n",
    "        filenames_image.sort()\n",
    "        print(\"Before removing labels direcotry from list\",filenames_image)\n",
    "        # Check if the label directory is in filenames_image and remove it from the list of filenames\n",
    "        if 'labels' in filenames_image:\n",
    "            filenames_image.remove('labels')\n",
    "        print(filenames_image)\n",
    "\n",
    "        filenames_label = os.listdir(label_dir)\n",
    "        filenames_label.sort()   \n",
    "\n",
    "        image_paths = [os.path.join(image_dir, filename) for filename in filenames_image]\n",
    "        label_paths = [os.path.join(label_dir, filename) for filename in filenames_label]\n",
    "\n",
    "        if len(filenames_image)!=len(filenames_label):\n",
    "                raise ValueError(\"The number of images and the number of labels is different. Please, check image_Tr and label_Tr folders.\")\n",
    "        \n",
    "        \n",
    "        # create json file - manually set\n",
    "\n",
    "        json_dict = OrderedDict()\n",
    "        json_dict['name'] = \"MRI Dataset - Choroid Plexus Segmentation\" \n",
    "        json_dict['description'] = self.description\n",
    "        json_dict['tensorImageSize'] = \"3D\"\n",
    "        json_dict['modality'] = {\n",
    "            \"0\": \"MR\"\n",
    "        }\n",
    "            \n",
    "        json_dict['labels'] = {\n",
    "            \"0\": \"background\",\n",
    "            \"1\": \"Choroid Plexus\"\n",
    "        }\n",
    "\n",
    "        json_dict['numTraining'] = len(image_paths)\n",
    "\n",
    "        json_dict['training'] = [{\"fold\": 0, \"image\": '%s' %i , \"label\": '%s' %j} for j, i in zip(label_paths, image_paths)]\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(json_dict[\"training\"])\n",
    "\n",
    "        # Split training data into N random folds\n",
    "        num_folds = 5\n",
    "        fold_size = len(json_dict[\"training\"]) // num_folds\n",
    "        for i in range(num_folds):\n",
    "            for j in range(fold_size):\n",
    "                json_dict[\"training\"][i * fold_size + j][\"fold\"] = i\n",
    "\n",
    "        with open(os.path.join(output_folder, json_filename), 'w') as f:\n",
    "                json.dump(json_dict, f, indent=4, sort_keys=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting launching_tool :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --dataroot DATAROOT\n",
      "                             [--description DESCRIPTION] --work_dir WORK_DIR\n",
      "                             [--training_dir TRAINING_DIR]\n",
      "                             [--train_json TRAIN_JSON]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataroot, --work_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class MyParser(argparse.ArgumentParser):\n",
    "    def error(self, message):\n",
    "        sys.stderr.write('error: %s\\n' % message)\n",
    "        self.print_help()\n",
    "        sys.exit(2)\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    print('Starting launching_tool :)')\n",
    "\n",
    "    # Initialize the parser\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Pipeline for training selected model from scratch or finetuning with N subjects with selected pretrained models\"\n",
    "    )\n",
    "\n",
    "    # Add the parameters positional/optional\n",
    "    parser.add_argument('--dataroot', required=True, default=\"/var/data/MONAI_Choroid_Plexus/dataset_train_from_scratch_monai\" , help=\"Data directory. Where the data is stored\")\n",
    "    parser.add_argument('--description', required=False, help=\"Data description\")\n",
    "    parser.add_argument('--work_dir', required=True, help=\"working directory\")\n",
    "    parser.add_argument('--training_dir', required=False, help=\"Working directory where to save trained models. If not specified, default folder name and locations will be used\")\n",
    "    parser.add_argument('--train_json', required=False, default=\"train.json\", help=\"Name of the train.json file\")\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    " \n",
    "    print('Writing JSON file for training.....')\n",
    "    json_file=WriteTrainJSON(args.dataroot, args.description, args.work_dir, args.training_dir).write_train_json(json_filename=args.train_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing labels direcotry from list ['0_ChP.nii.gz', '10_ChP.nii.gz', '11_ChP.nii.gz', '12_ChP.nii.gz', '13_ChP.nii.gz', '14_ChP.nii.gz', '15_ChP.nii.gz', '16_ChP.nii.gz', '17_ChP.nii.gz', '18_ChP.nii.gz', '19_ChP.nii.gz', '1_ChP.nii.gz', '20_ChP.nii.gz', '21_ChP.nii.gz', '22_ChP.nii.gz', '23_ChP.nii.gz', '24_ChP.nii.gz', '25_ChP.nii.gz', '26_ChP.nii.gz', '28_ChP.nii.gz', '29_ChP.nii.gz', '2_ChP.nii.gz', '3_ChP.nii.gz', '4_ChP.nii.gz', '5_ChP.nii.gz', '6_ChP.nii.gz', '7_ChP.nii.gz', '8_ChP.nii.gz', '9_ChP.nii.gz', 'labels']\n",
      "['0_ChP.nii.gz', '10_ChP.nii.gz', '11_ChP.nii.gz', '12_ChP.nii.gz', '13_ChP.nii.gz', '14_ChP.nii.gz', '15_ChP.nii.gz', '16_ChP.nii.gz', '17_ChP.nii.gz', '18_ChP.nii.gz', '19_ChP.nii.gz', '1_ChP.nii.gz', '20_ChP.nii.gz', '21_ChP.nii.gz', '22_ChP.nii.gz', '23_ChP.nii.gz', '24_ChP.nii.gz', '25_ChP.nii.gz', '26_ChP.nii.gz', '28_ChP.nii.gz', '29_ChP.nii.gz', '2_ChP.nii.gz', '3_ChP.nii.gz', '4_ChP.nii.gz', '5_ChP.nii.gz', '6_ChP.nii.gz', '7_ChP.nii.gz', '8_ChP.nii.gz', '9_ChP.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "json_file=WriteTrainJSON(\"/var/data/MONAI_Choroid_Plexus/dataset_train_from_scratch_monai\", work_dir = \"/var/data/student_home/lia/thesis/monai_segmentation/monai_training\").write_train_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2_ChP.nii.gz',\n",
       " '29_ChP.nii.gz',\n",
       " '20_ChP.nii.gz',\n",
       " '7_ChP.nii.gz',\n",
       " '14_ChP.nii.gz',\n",
       " '18_ChP.nii.gz',\n",
       " '3_ChP.nii.gz',\n",
       " 'labels',\n",
       " '25_ChP.nii.gz',\n",
       " '22_ChP.nii.gz',\n",
       " '9_ChP.nii.gz',\n",
       " '12_ChP.nii.gz',\n",
       " '8_ChP.nii.gz',\n",
       " '13_ChP.nii.gz',\n",
       " '23_ChP.nii.gz',\n",
       " '17_ChP.nii.gz',\n",
       " '19_ChP.nii.gz',\n",
       " '16_ChP.nii.gz',\n",
       " '24_ChP.nii.gz',\n",
       " '0_ChP.nii.gz',\n",
       " '28_ChP.nii.gz',\n",
       " '4_ChP.nii.gz',\n",
       " '26_ChP.nii.gz',\n",
       " '11_ChP.nii.gz',\n",
       " '21_ChP.nii.gz',\n",
       " '15_ChP.nii.gz',\n",
       " '1_ChP.nii.gz',\n",
       " '5_ChP.nii.gz',\n",
       " '10_ChP.nii.gz',\n",
       " '6_ChP.nii.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/var/data/MONAI_Choroid_Plexus/dataset_train_from_scratch_monai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
